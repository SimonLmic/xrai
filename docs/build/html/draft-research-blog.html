
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>XRAI - Hyperlocal AI for Spatial/Mobility Applications &#8212; XRAI  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to XRAI‚Äôs documentation!" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="xrai-hyperlocal-ai-for-spatial-mobility-applications">
<h1>XRAI - Hyperlocal AI for  Spatial/Mobility Applications<a class="headerlink" href="#xrai-hyperlocal-ai-for-spatial-mobility-applications" title="Permalink to this headline">¬∂</a></h1>
<p>Simon Micollier
May  , 2022</p>
<p><img alt="Capture" src="https://i.postimg.cc/nVKNCmsV/DHQ-copie.png" /></p>
<hr class="docutils" />
<p>XRAI technical promise page</p>
<hr class="docutils" />
<ul class="simple">
<li><p>üëâ <a class="reference external" href="https://www.halogem.co/">Try the XRAI demo </a></p></li>
<li><p>üëâ Visit the XRAI <a class="reference external" href="https://halogem.co/project">project page</a></p></li>
<li><p>üëÇ Try WebXR Sound Feedback <a class="reference external" href="https://rd.halogem.co/fb/arplayer">mobile web app</a> on 3D Voice Speaker</p></li>
<li><p>üëÅ Try WebXR Vision Feedback <a class="reference external" href="https://rd.halogem.co/fb/arplayer">mobile web app</a> on 3D Model Recommender</p></li>
<li><p>üíé Visit GEM Documentation <a class="reference external" href="https://threejs.org/docs/#api/en/renderers/webxr/WebXRManager">tutorial</a></p></li>
<li><p>üß† Try XRAI demo on <a class="reference external" href="https://colab.research.google.com/">Collab</a></p></li>
</ul>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
	<iframe src="https://youtube.com/shorts/wy6fkqO7ikQ" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¬∂</a></h2>
<p><em>We were able to run a specialized artificial intelligence Model trained on Hyperlocal datapoints issued by End-Users on our Immersive web application</em></p>
<p><em>We are the HelloFresh   for  AI-enabled  immersive digital reality : delivering data machines to Virtual Reality and Augmented Reality (VR/AR) humans to automatise their digital labor  through AI - keeping them in Fresh Digital Health !</em></p>
<p>Contextual note about Virtual Reality and Augmented Reality (VR/AR)  device to get situated in Y2022:</p>
<ul class="simple">
<li><p>Few VR devices have been shipped (Meta/FB communicate being ready to loose money to equip end users until 2025).</p></li>
<li><p>AR devices as Smart Glasses have not kicked off due to Optics display current specification limitation (and price). We believe this market is huge and Smart Phone tech will migrate to Smart Glasses to potentially extend or eventually replace it in Y2025.</p></li>
</ul>
<p>The way end-user consume digital content will radically change:  moving your attention to another screen to consume digital information is sub-optimal (we are unsafe/deshumanized walking phone in hands).</p>
<p>VR/AR apps combine the display of Digital and Real world together through unique programming interface to manage and develop audio/display, sensors &amp; inputs. By using it End-users can be faster, better, stronger and can experience the impossible.</p>
<p>This is a paradigm shift. Here is why we run Mobility/Spatial AI Deep Learning Model on top:</p>
<ol class="arabic simple">
<li><p>The value of all the Hyperlocal Data issued by Spatial applications through sensors and inputs is a key enabler for high accuracy and performance artificial intelligence.</p></li>
<li><p>We believe there is no robust solution on the market for this problem for now and to keep being Expert in that niche will requires a rare combination of skills (3D+AI+CREATIVE).</p></li>
<li><p>The value of being  capable to truely understand End-users and provide them contextualize high accuracy human-level AI enabled services directly from the inside.</p></li>
</ol>
<p>The Use of this technology in some business case can unlock emerging Services for BtoB/BtoBtoC or accelerate them :</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Task</p></th>
<th class="head"><p>Task Details</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Outdoor Pedestrians Tasks</p></td>
<td><p>Sport performance assistant; gamification,</p></td>
</tr>
<tr class="row-odd"><td><p>Health services Tasks</p></td>
<td><p>Surgeon Act Control and monitoring; Acrophobia detection,</p></td>
</tr>
<tr class="row-even"><td><p>Indoor services Tasks</p></td>
<td><p>Drone AI PathFinder; Super Humans augmented applications (audio translate, info context and a lot more undiscovered apps); Shop In-Person navigation</p></td>
</tr>
<tr class="row-odd"><td><p>Gaming &amp;Social services Tasks</p></td>
<td><p>AI Control and Monitoring automation to ensure both Users and Metaverse Site (Vendor‚Äôs content) is protected against harassment; behaviour out of policies, integrity and moderated exposition,</p></td>
</tr>
<tr class="row-even"><td><p>Immersive Payment services</p></td>
<td><p>Zero-Friction Virtual Card Payment; Transaction Control and Monitoring,</p></td>
</tr>
</tbody>
</table>
<p>Technically this business cases can already be implemented but will be driven by hardware (VR/AR devices) üçé.</p>
<p><strong>Problem</strong> : mobility/spatial deep learning as a service</p>
<p><strong>Solution</strong> : 100% API, We solve the problem with SAAS solutions specialized on this problem with a focus on high potential data-driven companies.</p>
<p><strong>Vision</strong> :</p>
<ul class="simple">
<li><p>Enabler for high accuracy Spatial AI compute</p></li>
<li><p>Bringing AI to XR to Enable intelligent meta wolds</p></li>
</ul>
<section id="key-props">
<h3>Key props<a class="headerlink" href="#key-props" title="Permalink to this headline">¬∂</a></h3>
<ul class="simple">
<li><p><strong>What is the given problem to solve?</strong></p>
<ul>
<li><p>Mobility/Spatial Deep Learning as a Service</p></li>
</ul>
</li>
<li><p><strong>Why we need to have it?</strong></p>
<ul>
<li><p>VR/AR applications are driven by  Sensors and Inputs to run, and can consequently unlock tremendous hyperlocal data on end-users device: this is the future of Software and applications</p></li>
<li><p>Consumer business they wont‚Äô consume all  the VR/AR content that is beautiful but that is not AI enabled</p></li>
<li><p>Corporate business they always wanted Performance and to replace manual work by machines</p></li>
<li><p>AI is already complex to implement technically in Business case: an AI specialization for VR/AR will be even more painful to implement for business as it combines different technical fields to fitin</p></li>
</ul>
</li>
<li><p><strong>Who will be using it?</strong></p>
<ul>
<li><p>Consumer business with Spaces in Metaverse with StopShop or Entertainment, bringing the value of AI and 3D through AI-enabled Spaces.</p></li>
<li><p>Corporate business with Digitized Spaces Warehouse or Medical with Operators/employee, bringing the value of AI and 3D through employee data to Control&amp;Monitoring operations.</p></li>
</ul>
</li>
<li><p><strong>How this will be achieved ?</strong></p>
<ul>
<li><p>Separating Technical and Functional,  technical expertise through 100% API solution,   implementing API in their business application</p></li>
<li><p>AI model specialization providing all the End to End AI Learning pipeline to accelerate : performant / scalable / multi model (Recommenders, Classifiers and Generators) XR dataset / Training challenge / Execution</p></li>
<li><p>Being Expert on that field and  moving fast in flow</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="room-task-example">
<h2>Room Task Example<a class="headerlink" href="#room-task-example" title="Permalink to this headline">¬∂</a></h2>
<p>VIDEO of the Tensor Core task FP16 resolution FP32 resolution</p>
</section>
<section id="key-features-or-capabilities">
<h2>Key Features or capabilities<a class="headerlink" href="#key-features-or-capabilities" title="Permalink to this headline">¬∂</a></h2>
<ul class="simple">
<li><p><strong>Hyper-Grid :</strong> Representing Real World &amp; Virtual Spaces as multi-dimensional grids in which your operators/employees and end-users operate: for example a digital twin parcel of an Hangar block, a Fantasy corner and a Virtual room in the Metaverse.</p></li>
<li><p><strong>XRAI Learnings :</strong> Training Model for the given First-Person user session samples to <em>[<strong>Think</strong>]</em> and <em>[<strong>Act</strong>]</em>  through   predictions, recommendations, contextual actions and choices along operator/end-user journey.</p></li>
<li><p><strong>XRAI Inference :</strong> Serving the results of AI computation for different applications including User situation detection in Mobility, Travel, Health, Security and Consumer use cases.</p></li>
<li><p><strong>XRAI Com Interface :</strong> <em>[<strong>Communicating</strong>]</em> Immersive Feedbacks to your Spatial application in  Digital Ears and Digital Eyes of your Users.</p></li>
</ul>
<p><img alt="Model Capabilities" src="https://lucid.app/publicSegments/view/2a4f2ae5-caea-4a00-8ced-51a654651e9e/image.png" /></p>
</section>
<section id="model-architecture">
<h2>Model Architecture<a class="headerlink" href="#model-architecture" title="Permalink to this headline">¬∂</a></h2>
<ul class="simple">
<li><p><strong>What is our Model given problem to solve ?</strong></p>
<ul>
<li><p>a performant and scalable specialization of AI to solve Mobility/Spatial Deep Learning as a service through Model capabilities applied to multiple business cases</p></li>
</ul>
</li>
<li><p><strong>Why we have to have  it?</strong></p>
<ul>
<li><p>Document Signal: Text, Audio and Image documents have their own structure: for example the Sound can be represented as a 2D wave and Image as 2D/3D Grids ‚Äì But for <strong>User Spatial Sessions</strong>* documents the more is different: User Spatial Sessions in our representation is a set of Spatial Datapoints <strong>6DOF</strong>* coordinates as End-Users are First-Person (camera) Datapoints:</p>
<ul>
<li><p>Each Datapoint  is NOT meaningful or expressive if considered one by one independently with an applied linear regression</p></li>
<li><p>So we had to connect each Datapoint all together as a Spatial Graph with our Hypergrid geometric data structure, to materialize documents structure for high-accuracy model training</p></li>
</ul>
</li>
<li><p>Bringing the best MODEL for spatial/mobility compute results which we reported here in this article through our AI specialization to accelerate that field</p></li>
</ul>
</li>
</ul>
<p>*<strong>6DOF</strong>:  6 degree of freedom when the user can browse digital/real world with a First-person view</p>
<p>*<strong>User Spatial Session</strong>: is  one activity in  the Metaverse which have a time start, a time end and one identifier</p>
<p><strong>XRAI Model</strong> is a trained Spatial AI model ; It uses Structural Geometric Neural Network (GraphNN) model as the backbone architecture, and other model for Signal Image, Text and Audio (for example Multimodal and pre-trained STT/TTS to send feedback to end-users) and it builds additional services on top of it . See the figure below for the overall model architecture for inference.</p>
<p><img alt="Model Architecture" src="https://lucid.app/publicSegments/view/db41eb70-db8b-4abf-839e-77dfa8a851a5/image.png" /></p>
<ul class="simple">
<li><p>Hypergrid is a data pre-processor separating spatial and non-spatial data:</p>
<ul>
<li><p>connect spatial Datapoints geometrically to output the Session graph for our specialized Model  processor</p></li>
<li><p>format not spatial Datapoints for a tabular data Model processor</p></li>
</ul>
</li>
</ul>
<p>XRAI is a  specialized Spatial AI model and a Model manager as it employs different deep-learning techniques together (Data Normalisation and Scaling pipelines, STT/TTS,  GAN,  Auto-Encoders, Recurrent NN) to achieve best performing Spatial AI computations. It is mainly build on GNN model. The XRAI Model is computation intensive so it supports accelerated hardware to run as XRAI  accelerate Mobility/Spatial AI Deep Learning through software specialization . However, its biggest weakness is the lack of end-user dataset.</p>
<p>XRAI improves on it by introducing tools and metrics to monitor and control all steps of the Training and Inference process: from data input, to feature, to training and inference: we have batch and real-time metrics output and Task follow-up to track our performance. So we can fast fix in case of convergence or quality drop by redeploying best model. This result in robust solution to keep in production best model for business-case MVP and performance Spatial AI Model as reported here.</p>
<ul class="simple">
<li><p><strong>Who will be using it ?</strong></p>
<ul>
<li><p>all spatial/mobility applications developers who want to focus on their business  and delegate all the complexity of Spatial AI to an API specialized on the intersection between VR/AR and AI.</p></li>
</ul>
</li>
<li><p><strong>How this will be achieved?</strong></p>
<ul>
<li><p>Spatial Data representation for Items (ID/SKU), Terrain (ID/SPACE) and User session (DATAPOINTS)</p></li>
<li><p>Spatial geometric operators (CONV3D) and XRAI Model (NN)</p></li>
<li><p>Training/Inference Tasks for 3D/6DOF AI Compute Model capabilities</p></li>
</ul>
</li>
</ul>
</section>
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">¬∂</a></h2>
<p>The mobile web app prototype we developed is a Progressive Web App (PWA) build on Three.js and WebXR, can be accessed on a WebXR enabled browser for IOS (for example  <a class="reference external" href="https://apps.apple.com/fr/app/webxr-viewer/id1295998056">Mozilla WebXR Viewer</a> ). We didn‚Äôt invested too much on the technical code build but it can be improved with Typescript and react-three-fiber.</p>
<p>The purpose of this development was to have a mobile application sandbox to collect user testing data because there is no good public Dataset for our Mobility and Spatial AI use case.</p>
<p>This prototype was looking like a Stop Shop supporting AR and VR to display a digital environment (3D Fantasy corner) and  a product (3D Product of the merchant).</p>
<p>The End-users can see some call to actions to Display or Add To Cart 3D Products (we did not implemented the Payment API) and further access to End-user Product Recommended/personalized Retail products like a motorcycle or a jacket.</p>
<p>The result of our development allowed us to output a Spatial interface for End-users with visual display, audio sound and our Think, Act &amp; Communicate API integrated with XRAI Model for inference.</p>
<ul class="simple">
<li><p><strong>What is our DATASET given problem to solve ?</strong></p>
<ul>
<li><p>User Spatial Sessions is a set of spatial Datapoint containing features:</p>
<ul>
<li><p>End-User Spatial Position coordinates in 6DOF space reference</p></li>
<li><p>End-User Digital Eye vector for example mobile camera direction and the intersection between user and tracked items</p></li>
<li><p>End-User Activities for example his interaction with Digital OR Real world</p></li>
</ul>
</li>
<li><p>User Sessions have not spatial data for example: User Agent,  OS and metadata.</p></li>
<li><p>Data was sent on the fly by our Mobile Application and saved in  MongoDB, so to built our Dataset we developed a database connector to download User Spatial Sessions issued by ~ 20 testing users.</p></li>
</ul>
</li>
<li><p><strong>Why we choose this Dataset instead of already existing one?</strong></p>
<ul>
<li><p>No public Dataset available earlier so we needed to have one</p></li>
<li><p>To capitalize in our experience to help you Bring your own Dataset to the platform and to challenge our Model</p></li>
</ul>
</li>
<li><p><strong>Who will be using it?</strong></p>
<ul>
<li><p>Spatial Application Developers commercializing VR/AR applications</p></li>
<li><p>Spatial Hardware Developers for specific industrial case (Drone/Healthcare or sport)</p></li>
</ul>
</li>
<li><p><strong>How we created this Dataset general technical &amp; functional data processing steps?</strong></p>
<ul>
<li><p>filtering the not representative sample</p></li>
<li><p>subset to challenge the most complex samples by clusters (for example: similar user sessions for In-Shop User Session time spent between 10 and 20 sec that have only see product with no digital interactions with the product of the vendor)</p></li>
<li><p>blacklisting personal privacy features</p></li>
<li><p>normalizing sample features</p></li>
</ul>
</li>
</ul>
<p><strong>Dataset developer form:</strong></p>
<p>If you are developing Mobility/Spatial Application hang with us ü¶Ñ :</p>
<p><img alt="Dataset" src="https://i.postimg.cc/wTSJRSYp/te-le-chargement.png" /></p>
<p>Here is some reference to help you get started to make your own Dataset:</p>
<ul class="simple">
<li><p>WebXR <a class="reference external" href="https://immersive-web.github.io/webxr/explainer.html">Explainer</a></p></li>
<li><p>WebXR for your IOS application with <a class="reference external" href="https://apps.apple.com/fr/app/webxr-viewer/id1295998056">Mozilla WebXR Viewer</a> and Android mobiles</p></li>
<li><p>WebXR for your <a class="reference external" href="https://skarredghost.com/2022/01/05/how-to-oculus-spatial-anchors-unity-2/">Oculus Quest</a> with <a class="reference external" href="https://web.dev/pwas-on-oculus-2/">PWA‚Äôs Web application</a></p></li>
<li><p>XR for your Device OS with <a class="reference external" href="https://docs.unity3d.com/Manual/XR.html">Unity XR</a>, ARKIT, ARCORE</p></li>
</ul>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¬∂</a></h2>
<ul class="simple">
<li><p><strong>What is the given problem for Training to Solve?</strong></p>
<ul>
<li><p>Training is performed against multiple Referent Spaces and further requires generalization to run inference against any Referent Spaces</p></li>
<li><p>Poly-Data (polymorphism) due to the various End-User device and sensors, Data requires to be preped for Training</p></li>
<li><p>Geometric (6DOF) neural network is Ressource-intensive</p></li>
</ul>
</li>
<li><p><strong>Why we have to solve Training for this problem?</strong></p>
<ul>
<li><p>User Session Graph for End-User 1 in User Referent Space 1 with a SmartPhones train the same model as User Session Graph for End-User 2 in User Referent-Space 2 with SmartGlasses</p></li>
<li><p>Training for 6DOF Neural Network is Ressource-intensive</p></li>
<li><p>Have training robust to scale</p></li>
</ul>
</li>
<li><p><strong>Who will be using it?</strong></p>
<ul>
<li><p>Data Scientist and Developers</p></li>
</ul>
</li>
<li><p><strong>How this will be achieved?</strong></p>
<ul>
<li><p>Training is easy to manage its like Continuous Training for Data-driven AI accelerated system</p></li>
<li><p>With Continuous Monitoring/Alerting for all the Training Metrics we think are relevant for our AI specialization (and against baseline).</p></li>
<li><p>Space segmentation, Space invariance through normalization and precision resolution adjust.</p></li>
<li><p>Fine tuning is possible by Training the model on more steps for a particular Dataset clusters or a new Dataset</p></li>
<li><p>Model hyper parameters is possible through a model structure file definition</p></li>
<li><p>Model Flow with parallelization, saving and deployment</p></li>
<li><p>Model Configuration with  batch size (3),  optimizer (Adam) , learning rate gamma (0.001), weight decay (0.01)</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<p>That is a End-User User Reference Space :
<img alt="sim" src="https://i.postimg.cc/qvyqWQbX/Capture-d-e-cran-2022-02-03-a-17-20-05.png" /></p>
<p>That is a Digital Reconstruction :
<img alt="sim" src="https://i.postimg.cc/Y9ZjJVHR/Capture-d-e-cran-2022-02-03-a-17-17-47.png" /></p>
<p>That is a End-User User Session (and User Act)  :
<img alt="sim" src="https://i.postimg.cc/SxfKtkPy/all-sessions.png" /></p>
<p>That is a End-User User Session and User Act Precision Resolution processing :
<img alt="enter image description here" src="https://i.postimg.cc/T36w9pC7/all-voxels.png" /></p>
<p>That resulted Data Structure is converted into a Geometric Graph Node and our algorithm handle the complexity of the adjacency graph.</p>
<p>Graph data structure is then directly ingested by our Graph Neural Network Model .</p>
</section>
<section id="api">
<h2>API<a class="headerlink" href="#api" title="Permalink to this headline">¬∂</a></h2>
<ul class="simple">
<li><p>Swag it</p></li>
</ul>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API Endpoints</p></th>
<th class="head"><p>Developer Package</p></th>
<th class="head"><p>Raw Data</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img alt="enter image description here" src="https://i.postimg.cc/FzY3S39W/Capture-d-e-cran-2022-05-09-a-21-26-35.png" /></p></td>
<td><p><img alt="enter image description here" src="https://i.postimg.cc/nry5Yp5M/Capture-d-e-cran-2022-05-18-a-21-27-57.png" /></p></td>
<td><p><img alt="enter image description here" src="https://i.postimg.cc/bJbFyKdh/Capture-d-e-cran-2022-05-18-a-21-31-32.png" /></p></td>
</tr>
</tbody>
</table>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span>  <span class="nc">ModelAPI</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="nd">@abstractmethod</span>
<span class="k">def</span>  <span class="nf">loadModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="k">return</span>

<span class="nd">@abstractmethod</span>
<span class="k">def</span>  <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="k">return</span>


<span class="nd">@abstractmethod</span>
<span class="k">def</span>  <span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="k">return</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="sd">@abstractmethod</span>
<span class="sd">def load_checkpoint():</span>
<span class="sd">return</span>

<span class="sd">@abstractmethod</span>
<span class="sd">def eval_step():</span>
<span class="sd">return</span>

<span class="sd">@abstractmethod</span>
<span class="sd">def save_model():</span>
<span class="sd">return</span>

<span class="sd">@abstractmethod</span>
<span class="sd">def test_run():</span>
<span class="sd">return</span>

<span class="sd">@abstractmethod</span>
<span class="sd">def eval_model_performance():</span>
<span class="sd">return</span>

<span class="sd">@abstractmethod</span>
<span class="sd">def train_log():</span>
<span class="sd">return</span>

<span class="sd">@abstractmethod</span>
<span class="sd">def build_report():</span>
<span class="sd">return</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="k">class</span>  <span class="nc">VisionModelAPI</span><span class="p">(</span><span class="n">ModelAPI</span><span class="p">):</span>
	<span class="k">def</span>  <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">ModelAPI</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
		
	<span class="k">def</span>  <span class="nf">loadModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
		<span class="kn">from</span>  <span class="nn">src.serve</span>  <span class="kn">import</span>  <span class="n">get_bundle</span><span class="p">,</span> <span class="n">get_model</span><span class="p">,</span> <span class="n">device</span>
		<span class="n">bundle</span> <span class="o">=</span> <span class="n">get_bundle</span><span class="p">()</span>
		<span class="n">model</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="n">bundle</span><span class="p">)</span>
		<span class="k">return</span>  <span class="n">bundle</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span>

	<span class="k">def</span>  <span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
		<span class="kn">from</span>  <span class="nn">src.serve</span>  <span class="kn">import</span>  <span class="n">inference</span>
		<span class="n">out</span> <span class="o">=</span> <span class="n">inference</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
		<span class="k">return</span>  <span class="n">out</span>

	<span class="k">def</span>  <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">views_input_file</span><span class="p">,</span> <span class="n">engaged_input_file</span><span class="p">):</span>
		<span class="kn">from</span>  <span class="nn">src.XRPredict</span>  <span class="kn">import</span>  <span class="n">train_model_checkpoint</span>
		<span class="n">model</span> <span class="o">=</span> <span class="n">train_model_checkpoint</span><span class="p">(</span><span class="n">views_input_file</span><span class="p">,</span><span class="n">engaged_input_file</span><span class="p">)</span>
		<span class="k">return</span>


<span class="k">class</span>  <span class="nc">AudioModelAPI</span><span class="p">(</span><span class="n">ModelAPI</span><span class="p">):</span>
	<span class="k">def</span>  <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">ModelAPI</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

	<span class="k">def</span>  <span class="nf">loadModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
		<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span>  <span class="k">if</span>  <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span>  <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
		<span class="n">bundle</span> <span class="o">=</span> <span class="n">torchaudio</span><span class="o">.</span><span class="n">pipelines</span><span class="o">.</span><span class="n">WAV2VEC2_ASR_BASE_960H</span>
		<span class="n">model</span> <span class="o">=</span> <span class="n">bundle</span><span class="o">.</span><span class="n">get_model</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
		<span class="k">return</span>  <span class="n">bundle</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span>

	<span class="k">def</span>  <span class="nf">defineModelSpec</span><span class="p">(</span><span class="n">spec</span><span class="p">):</span>
		<span class="k">class</span>  <span class="nc">GreedyCTCDecoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
			<span class="k">def</span>  <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
				<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">blank</span> <span class="o">=</span> <span class="n">blank</span>

			<span class="k">def</span>  <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emission</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
				<span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">emission</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [num_seq,]</span>
				<span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
				<span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span>  <span class="k">for</span>  <span class="n">i</span>  <span class="ow">in</span>  <span class="n">indices</span>  <span class="k">if</span>  <span class="n">i</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blank</span><span class="p">]</span>
				<span class="k">return</span>  <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span>  <span class="n">i</span>  <span class="ow">in</span>  <span class="n">indices</span><span class="p">])</span>
		<span class="k">return</span>  <span class="n">GreedyCTCDecoder</span>
		
	<span class="k">def</span>  <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
	<span class="k">return</span>

	<span class="k">def</span>  <span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
	<span class="k">return</span>

	<span class="k">def</span>  <span class="nf">vec_matching</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_1</span><span class="p">,</span> <span class="n">input_2</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
	<span class="sd">&#39;&#39;&#39;compute distance between one vectorized input 1 and persisted index</span>
<span class="sd">	optional : can compute distance between vector1 and vector2</span>
<span class="sd">	return top-k matchs&#39;&#39;&#39;</span>

</pre></div>
</div>
</section>
<section id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¬∂</a></h2>
<ul class="simple">
<li><p>what is the given problem to solve</p>
<ul>
<li><p>what metric we used</p>
<ul>
<li><p>accuracy</p></li>
<li><p>confusion matrix</p></li>
<li><p>loss</p></li>
<li><p>auc/roc</p></li>
<li><p>precision</p></li>
<li><p>recall</p></li>
<li><p>f1</p></li>
</ul>
</li>
<li><p>tracked/optimized</p></li>
<li><p>other dataset</p></li>
</ul>
</li>
<li><p>why we want to solve it</p>
<ul>
<li><p>j</p></li>
</ul>
</li>
<li><p>who will be using it</p>
<ul>
<li><p>j</p></li>
</ul>
</li>
<li><p>how this will be achieved</p>
<ul>
<li><p>using dataset</p></li>
<li><p>using monitoring tool</p></li>
<li><p>using scale backend Python/Pytorch FastAPI</p></li>
</ul>
</li>
</ul>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Dataset1</p></th>
<th class="head"><p></p></th>
<th class="head"><p>Dataset2</p></th>
<th class="head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Exp.</p></td>
<td><p>Accuracy</p></td>
<td><p>F1</p></td>
<td><p>Accuracy</p></td>
<td><p>F1</p></td>
</tr>
<tr class="row-odd"><td><p>DecisionTree</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
</tr>
<tr class="row-even"><td><p>NN</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
</tr>
<tr class="row-odd"><td><p>Groundtruth</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
</tr>
<tr class="row-even"><td><p>Exp1</p></td>
<td><p>0.81</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
</tr>
<tr class="row-odd"><td><p>Exp1 + Finetuning</p></td>
<td><p>0.91</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
</tr>
<tr class="row-even"><td><p>Exp2</p></td>
<td><p>0.96</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
<td><p>üåÆ</p></td>
</tr>
</tbody>
</table>
<p><img alt="enter image description here" src="https://i.postimg.cc/QdTQB9Vf/Screenshot-from-2022-02-15-15-56-31.png" /></p>
<p>Due to the time and space constraints in the project, we could not expand the experiments to all the possible use-cases of XRAI. We plan to include those in our future study and add new capabilities to XRAI that would give more</p>
</section>
<section id="try-out">
<h2>Try out<a class="headerlink" href="#try-out" title="Permalink to this headline">¬∂</a></h2>
</section>
<section id="ethical-concerns">
<h2>Ethical Concerns<a class="headerlink" href="#ethical-concerns" title="Permalink to this headline">¬∂</a></h2>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tracking can be sensitive to local regulation but our hyperlocal approach based on anonymous hyperlocal data is effective and can help to capture end-user behaviours and profiles.
</pre></div>
</div>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¬∂</a></h2>
<p>For AI-Enabled companies, have to migrate their AI implementation into the new VR/AR paradigm shift: new architecture, new product, new compute cluster and so it‚Äôs faster to provide them with fast to implement AI solutions</p>
<p>For Not AI-Enabled companies: have to enable AI for their Business cases including direct VR/AR AI Deep Learning solutions.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">XRAI</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">XRAI - Hyperlocal AI for  Spatial/Mobility Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#room-task-example">Room Task Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="#key-features-or-capabilities">Key Features or capabilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-architecture">Model Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dataset">Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#api">API</a></li>
<li class="toctree-l2"><a class="reference internal" href="#results">Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="#try-out">Try out</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ethical-concerns">Ethical Concerns</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Welcome to XRAI‚Äôs documentation!</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, Halogem.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.5.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/draft-research-blog.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>