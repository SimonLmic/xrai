# XRAI - Hyperlocal AI for  Spatial/Mobility Applications

Simon Micollier
May  , 2022


![Capture](https://i.postimg.cc/nVKNCmsV/DHQ-copie.png)

---
XRAI technical promise page

---

- 👉 [Try the XRAI demo ](https://www.halogem.co/)
- 👉 Visit the XRAI [project page](https://halogem.co/project)
- 👂 Try WebXR Sound Feedback [mobile web app](https://rd.halogem.co/fb/arplayer) on 3D Voice Speaker
- 👁 Try WebXR Vision Feedback [mobile web app](https://rd.halogem.co/fb/arplayer) on 3D Model Recommender
- 💎 Visit GEM Documentation [tutorial](https://threejs.org/docs/#api/en/renderers/webxr/WebXRManager)
- 🧠 Try XRAI demo on [Collab](https://colab.research.google.com/)


<iframe src="https://youtube.com/embed/wy6fkqO7ikQ" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>

## Introduction

*We were able to run a specialized artificial intelligence Model trained on Hyperlocal datapoints issued by End-Users on our Immersive web application* 

*We are the HelloFresh   for  AI-enabled  immersive digital reality : delivering data machines to Virtual Reality and Augmented Reality (VR/AR) humans to automatise their digital labor  through AI - keeping them in Fresh Digital Health !* 

Contextual note about Virtual Reality and Augmented Reality (VR/AR)  device to get situated in Y2022: 
- Few VR devices have been shipped (Meta/FB communicate being ready to loose money to equip end users until 2025).
- AR devices as Smart Glasses have not kicked off due to Optics display current specification limitation (and price). We believe this market is huge and Smart Phone tech will migrate to Smart Glasses to potentially extend or eventually replace it in Y2025.

The way end-user consume digital content will radically change:  moving your attention to another screen to consume digital information is sub-optimal (we are unsafe/deshumanized walking phone in hands).

VR/AR apps combine the display of Digital and Real world together through unique programming interface to manage and develop audio/display, sensors & inputs. By using it End-users can be faster, better, stronger and can experience the impossible.

This is a paradigm shift. Here is why we run Mobility/Spatial AI Deep Learning Model on top:  
 1. The value of all the Hyperlocal Data issued by Spatial applications through sensors and inputs is a key enabler for high accuracy and performance artificial intelligence.
 2. We believe there is no robust solution on the market for this problem for now and to keep being Expert in that niche will requires a rare combination of skills (3D+AI+CREATIVE).
 3. The value of being  capable to truely understand End-users and provide them contextualize high accuracy human-level AI enabled services directly from the inside.


The Use of this technology in some business case can unlock emerging Services for BtoB/BtoBtoC or accelerate them :

| Task | Task Details |
|--|--|
| Outdoor Pedestrians Tasks |Sport performance assistant; gamification, |
| Health services Tasks |Surgeon Act Control and monitoring; Acrophobia detection,
| Indoor services Tasks | Drone AI PathFinder; Super Humans augmented applications (audio translate, info context and a lot more undiscovered apps); Shop In-Person navigation |
| Gaming &Social services Tasks| AI Control and Monitoring automation to ensure both Users and Metaverse Site (Vendor's content) is protected against harassment; behaviour out of policies, integrity and moderated exposition,| 
| Immersive Payment services | Zero-Friction Virtual Card Payment; Transaction Control and Monitoring, | 

Technically this business cases can already be implemented but will be driven by hardware (VR/AR devices) 🍎.

**Problem** : mobility/spatial deep learning as a service 

**Solution** : 100% API, We solve the problem with SAAS solutions specialized on this problem with a focus on high potential data-driven companies. 

**Vision** :
- Enabler for high accuracy Spatial AI compute
- Bringing AI to XR to Enable intelligent meta wolds


### Key props

- **What is the given problem to solve?**  
	- Mobility/Spatial Deep Learning as a Service
- **Why we need to have it?**
	- VR/AR applications are driven by  Sensors and Inputs to run, and can consequently unlock tremendous hyperlocal data on end-users device: this is the future of Software and applications 
	-  Consumer business they wont' consume all  the VR/AR content that is beautiful but that is not AI enabled
	- Corporate business they always wanted Performance and to replace manual work by machines
	-  AI is already complex to implement technically in Business case: an AI specialization for VR/AR will be even more painful to implement for business as it combines different technical fields to fitin

- **Who will be using it?**
	- Consumer business with Spaces in Metaverse with StopShop or Entertainment, bringing the value of AI and 3D through AI-enabled Spaces.
	- Corporate business with Digitized Spaces Warehouse or Medical with Operators/employee, bringing the value of AI and 3D through employee data to Control&Monitoring operations.

- **How this will be achieved ?**
	- Separating Technical and Functional,  technical expertise through 100% API solution,   implementing API in their business application 
	- AI model specialization providing all the End to End AI Learning pipeline to accelerate : performant / scalable / multi model (Recommenders, Classifiers and Generators) XR dataset / Training challenge / Execution 
	- Being Expert on that field and  moving fast in flow


## Room Task Example 


VIDEO of the Tensor Core task FP16 resolution FP32 resolution


## Key Features or capabilities

- **Hyper-Grid :** Representing Real World & Virtual Spaces as multi-dimensional grids in which your operators/employees and end-users operate: for example a digital twin parcel of an Hangar block, a Fantasy corner and a Virtual room in the Metaverse. 
- **XRAI Learnings :** Training Model for the given First-Person user session samples to *[**Think**]* and *[**Act**]*  through   predictions, recommendations, contextual actions and choices along operator/end-user journey.
- **XRAI Inference :** Serving the results of AI computation for different applications including User situation detection in Mobility, Travel, Health, Security and Consumer use cases.
- **XRAI Com Interface :** *[**Communicating**]* Immersive Feedbacks to your Spatial application in  Digital Ears and Digital Eyes of your Users.


![Model Capabilities](https://lucid.app/publicSegments/view/2a4f2ae5-caea-4a00-8ced-51a654651e9e/image.png)


## Model Architecture

- **What is our Model given problem to solve ?**
	- a performant and scalable specialization of AI to solve Mobility/Spatial Deep Learning as a service through Model capabilities applied to multiple business cases
- **Why we have to have  it?**
	- Document Signal: Text, Audio and Image documents have their own structure: for example the Sound can be represented as a 2D wave and Image as 2D/3D Grids -- But for **User Spatial Sessions*** documents the more is different: User Spatial Sessions in our representation is a set of Spatial Datapoints **6DOF*** coordinates as End-Users are First-Person (camera) Datapoints:
		- Each Datapoint  is NOT meaningful or expressive if considered one by one independently with an applied linear regression
		- So we had to connect each Datapoint all together as a Spatial Graph with our Hypergrid geometric data structure, to materialize documents structure for high-accuracy model training
	- Bringing the best MODEL for spatial/mobility compute results which we reported here in this article through our AI specialization to accelerate that field

***6DOF**:  6 degree of freedom when the user can browse digital/real world with a First-person view

***User Spatial Session**: is  one activity in  the Metaverse which have a time start, a time end and one identifier 

**XRAI Model** is a trained Spatial AI model ; It uses Structural Geometric Neural Network (GraphNN) model as the backbone architecture, and other model for Signal Image, Text and Audio (for example Multimodal and pre-trained STT/TTS to send feedback to end-users) and it builds additional services on top of it . See the figure below for the overall model architecture for inference. 

![Model Architecture](https://lucid.app/publicSegments/view/db41eb70-db8b-4abf-839e-77dfa8a851a5/image.png)
- Hypergrid is a data pre-processor separating spatial and non-spatial data:
	- connect spatial Datapoints geometrically to output the Session graph for our specialized Model  processor
	- format not spatial Datapoints for a tabular data Model processor

 XRAI is a  specialized Spatial AI model and a Model manager as it employs different deep-learning techniques together (Data Normalisation and Scaling pipelines, STT/TTS,  GAN,  Auto-Encoders, Recurrent NN) to achieve best performing Spatial AI computations. It is mainly build on GNN model. The XRAI Model is computation intensive so it supports accelerated hardware to run as XRAI  accelerate Mobility/Spatial AI Deep Learning through software specialization . However, its biggest weakness is the lack of end-user dataset. 

XRAI improves on it by introducing tools and metrics to monitor and control all steps of the Training and Inference process: from data input, to feature, to training and inference: we have batch and real-time metrics output and Task follow-up to track our performance. So we can fast fix in case of convergence or quality drop by redeploying best model. This result in robust solution to keep in production best model for business-case MVP and performance Spatial AI Model as reported here.

- **Who will be using it ?**
	- all spatial/mobility applications developers who want to focus on their business  and delegate all the complexity of Spatial AI to an API specialized on the intersection between VR/AR and AI.

- **How this will be achieved?**
	- Spatial Data representation for Items (ID/SKU), Terrain (ID/SPACE) and User session (DATAPOINTS)
	- Spatial geometric operators (CONV3D) and XRAI Model (NN)
	- Training/Inference Tasks for 3D/6DOF AI Compute Model capabilities  

## Dataset

The mobile web app prototype we developed is a Progressive Web App (PWA) build on Three.js and WebXR, can be accessed on a WebXR enabled browser for IOS (for example  [Mozilla WebXR Viewer](https://apps.apple.com/fr/app/webxr-viewer/id1295998056) ). We didn't invested too much on the technical code build but it can be improved with Typescript and react-three-fiber. 

The purpose of this development was to have a mobile application sandbox to collect user testing data because there is no good public Dataset for our Mobility and Spatial AI use case.

This prototype was looking like a Stop Shop supporting AR and VR to display a digital environment (3D Fantasy corner) and  a product (3D Product of the merchant).

The End-users can see some call to actions to Display or Add To Cart 3D Products (we did not implemented the Payment API) and further access to End-user Product Recommended/personalized Retail products like a motorcycle or a jacket.

The result of our development allowed us to output a Spatial interface for End-users with visual display, audio sound and our Think, Act & Communicate API integrated with XRAI Model for inference.

- **What is our DATASET given problem to solve ?**
	- User Spatial Sessions is a set of spatial Datapoint containing features:
		- End-User Spatial Position coordinates in 6DOF space reference
		- End-User Digital Eye vector for example mobile camera direction and the intersection between user and tracked items
		- End-User Activities for example his interaction with Digital OR Real world
	-  User Sessions have not spatial data for example: User Agent,  OS and metadata.
	- Data was sent on the fly by our Mobile Application and saved in  MongoDB, so to built our Dataset we developed a database connector to download User Spatial Sessions issued by ~ 20 testing users.
- **Why we choose this Dataset instead of already existing one?**
	- No public Dataset available earlier so we needed to have one
	- To capitalize in our experience to help you Bring your own Dataset to the platform and to challenge our Model
- **Who will be using it?** 
	- Spatial Application Developers commercializing VR/AR applications
	- Spatial Hardware Developers for specific industrial case (Drone/Healthcare or sport)
- **How we created this Dataset general technical & functional data processing steps?**
	- filtering the not representative sample
	- subset to challenge the most complex samples by clusters (for example: similar user sessions for In-Shop User Session time spent between 10 and 20 sec that have only see product with no digital interactions with the product of the vendor)
	- blacklisting personal privacy features
	- normalizing sample features

**Dataset developer form:**
 
If you are developing Mobility/Spatial Application hang with us 🦄 :

![Dataset](https://i.postimg.cc/wTSJRSYp/te-le-chargement.png)

Here is some reference to help you get started to make your own Dataset:
- WebXR [Explainer](https://immersive-web.github.io/webxr/explainer.html)
- WebXR for your IOS application with [Mozilla WebXR Viewer](https://apps.apple.com/fr/app/webxr-viewer/id1295998056) and Android mobiles
- WebXR for your [Oculus Quest](https://skarredghost.com/2022/01/05/how-to-oculus-spatial-anchors-unity-2/) with [PWA's Web application](https://web.dev/pwas-on-oculus-2/) 
- XR for your Device OS with [Unity XR](https://docs.unity3d.com/Manual/XR.html), ARKIT, ARCORE


## Training

- **What is the given problem for Training to Solve?** 
	- Training is performed against multiple Referent Spaces and further requires generalization to run inference against any Referent Spaces
	- Poly-Data (polymorphism) due to the various End-User device and sensors, Data requires to be preped for Training
	-  Geometric (6DOF) neural network is Ressource-intensive

- **Why we have to solve Training for this problem?**
	-  User Session Graph for End-User 1 in User Referent Space 1 with a SmartPhones train the same model as User Session Graph for End-User 2 in User Referent-Space 2 with SmartGlasses
	- Training for 6DOF Neural Network is Ressource-intensive 
	- Have training robust to scale

- **Who will be using it?**
	- Data Scientist and Developers
	
- **How this will be achieved?**
	- Training is easy to manage its like Continuous Training for Data-driven AI accelerated system    
	- With Continuous Monitoring/Alerting for all the Training Metrics we think are relevant for our AI specialization (and against baseline).
	- Space segmentation, Space invariance through normalization and precision resolution adjust. 
	- Fine tuning is possible by Training the model on more steps for a particular Dataset clusters or a new Dataset
	- Model hyper parameters is possible through a model structure file definition
	- Model Flow with parallelization, saving and deployment
	- Model Configuration with  batch size (3),  optimizer (Adam) , learning rate gamma (0.001), weight decay (0.01)

---
That is a End-User User Reference Space :
![sim](https://i.postimg.cc/qvyqWQbX/Capture-d-e-cran-2022-02-03-a-17-20-05.png)

That is a Digital Reconstruction :
![sim](https://i.postimg.cc/Y9ZjJVHR/Capture-d-e-cran-2022-02-03-a-17-17-47.png)


That is a End-User User Session (and User Act)  :
![sim](https://i.postimg.cc/SxfKtkPy/all-sessions.png) 

That is a End-User User Session and User Act Precision Resolution processing : 
![enter image description here](https://i.postimg.cc/T36w9pC7/all-voxels.png)

That resulted Data Structure is converted into a Geometric Graph Node and our algorithm handle the complexity of the adjacency graph.

Graph data structure is then directly ingested by our Graph Neural Network Model .


## API 

- Swag it


|API Endpoints | Developer Package | Raw Data|
|--|--|--|
|![enter image description here](https://i.postimg.cc/FzY3S39W/Capture-d-e-cran-2022-05-09-a-21-26-35.png)|![enter image description here](https://i.postimg.cc/nry5Yp5M/Capture-d-e-cran-2022-05-18-a-21-27-57.png) | ![enter image description here](https://i.postimg.cc/bJbFyKdh/Capture-d-e-cran-2022-05-18-a-21-31-32.png) |

```python 
class  ModelAPI(ABC):
@abstractmethod
def  loadModel(self, input):
"""
"""
return

@abstractmethod
def  train(self, input):
"""
"""
return


@abstractmethod
def  inference(self, input):
"""
"""
return

"""

@abstractmethod
def load_checkpoint():
return

@abstractmethod
def eval_step():
return

@abstractmethod
def save_model():
return

@abstractmethod
def test_run():
return

@abstractmethod
def eval_model_performance():
return

@abstractmethod
def train_log():
return

@abstractmethod
def build_report():
return
"""

class  VisionModelAPI(ModelAPI):
	def  __init__(self):
		super(ModelAPI, self).__init__()
		
	def  loadModel(self, model=None):
		from  src.serve  import  get_bundle, get_model, device
		bundle = get_bundle()
		model = get_model(bundle)
		return  bundle, model, device

	def  inference(self, input):
		from  src.serve  import  inference
		out = inference(input)
		return  out

	def  train(self, views_input_file, engaged_input_file):
		from  src.XRPredict  import  train_model_checkpoint
		model = train_model_checkpoint(views_input_file,engaged_input_file)
		return


class  AudioModelAPI(ModelAPI):
	def  __init__(self):
		super(ModelAPI, self).__init__()

	def  loadModel(self, model=None):
		device = torch.device('cuda'  if  torch.cuda.is_available() else  'cpu')
		bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H
		model = bundle.get_model().to(device)
		return  bundle, model, device

	def  defineModelSpec(spec):
		class  GreedyCTCDecoder(torch.nn.Module):
			def  __init__(self, labels, blank=0):
				super().__init__()
				self.labels = labels
				self.blank = blank

			def  forward(self, emission: torch.Tensor) -> str:
				indices = torch.argmax(emission, dim=-1) # [num_seq,]
				indices = torch.unique_consecutive(indices, dim=-1)
				indices = [i  for  i  in  indices  if  i != self.blank]
				return  ''.join([self.labels[i] for  i  in  indices])
		return  GreedyCTCDecoder
		
	def  train(self):
	return

	def  inference(self, input):
	return

	def  vec_matching(self, input_1, input_2=None):
	'''compute distance between one vectorized input 1 and persisted index
	optional : can compute distance between vector1 and vector2
	return top-k matchs'''

```



## Results

- what is the given problem to solve 
	- what metric we used
		- accuracy
		- confusion matrix
		- loss
		- auc/roc
		- precision
		- recall
		- f1
	- tracked/optimized
	- other dataset 
- why we want to solve it 
	- j
- who will be using it 
	- j
- how this will be achieved
	- using dataset
	- using monitoring tool  
	- using scale backend Python/Pytorch FastAPI


|  | Dataset1 | |Dataset2| |
|--|--|--|--|--|
| Exp.  | Accuracy | F1 | Accuracy | F1 |
| DecisionTree  | 🌮 | 🌮| 🌮| 🌮 |
| NN  | 🌮 | 🌮 | 🌮| 🌮 |
| Groundtruth  | 🌮 | 🌮 | 🌮| 🌮 | 
| Exp1  | 0.81 | 🌮 | 🌮| 🌮 | 
| Exp1 + Finetuning | 0.91 | 🌮 | 🌮| 🌮 |
| Exp2  | 0.96| 🌮 | 🌮| 🌮 | 


![enter image description here](https://i.postimg.cc/QdTQB9Vf/Screenshot-from-2022-02-15-15-56-31.png)

Due to the time and space constraints in the project, we could not expand the experiments to all the possible use-cases of XRAI. We plan to include those in our future study and add new capabilities to XRAI that would give more 


## Try out 



## Ethical Concerns

	Tracking can be sensitive to local regulation but our hyperlocal approach based on anonymous hyperlocal data is effective and can help to capture end-user behaviours and profiles.


## Conclusion


For AI-Enabled companies, have to migrate their AI implementation into the new VR/AR paradigm shift: new architecture, new product, new compute cluster and so it's faster to provide them with fast to implement AI solutions

For Not AI-Enabled companies: have to enable AI for their Business cases including direct VR/AR AI Deep Learning solutions.

